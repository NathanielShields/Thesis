{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code inherited from christianversloot (github)'s machine learning articles and Deep Learning in Python (Chollet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the sys module\n",
    "import sys\n",
    "sys.path.append('C:/Users/nsshi/github/ThesisFinal/src/modules')  \n",
    "import lib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# File data\n",
    "save_dir = '/modelsOut/'\n",
    "emg_filename = r'Data\\EMG_Mar_19_09_34_15.txt'\n",
    "q_filename = r'Data\\Q_Mar_19_09_33_45.txt'\n",
    "\n",
    "# Preprocessing\n",
    "emg, emgTimes, q, qTimes = lib.import_files(emg_filename, q_filename, old=True)\n",
    "qF, emgF, tF, timestampsF = lib.unify_timeseries_high(emg, emgTimes, q, qTimes)\n",
    "emgModel = lib.normalize_emg_rolling(emgF)\n",
    "qModel = lib.finger_nodes(qF, finger = 'indextip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 25\n",
    "trn, val, tst, input_shape, output_shape, n_features = lib.create_feature_dataset(tF, emgModel, qModel, sequence_length = sequence_length, stride_features=10, full = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_rmse_val, naive_rmse_tst = lib.naive_method(qModel)\n",
    "print(\"Naive Validation Error:\", naive_rmse_val)\n",
    "print(\"Naive Test Error:\", naive_rmse_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Dense Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "\n",
    "# Dense layers\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "x = layers.Dense(8, activation=\"relu\")(x)\n",
    "x = layers.Dense(4, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_basic_dense.keras\",\n",
    " save_best_only=True)\n",
    "] \n",
    "model.compile(optimizer = \"rmsprop\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs = 10,\n",
    " validation_data = val,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_basic_dense.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex model\n",
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(1200, activation=\"relu\")(x)\n",
    "x = layers.Dense(100, activation=\"relu\")(x)\n",
    "x = layers.Dense(10, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_heavy_dense.keras\",\n",
    " save_best_only=True)\n",
    "] \n",
    "model.compile(optimizer = \"rmsprop\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs = 5,\n",
    " validation_data = val,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_heavy_dense.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even more complex model\n",
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(1200, activation=\"relu\")(x)\n",
    "x = layers.Dense(600, activation=\"relu\")(x)\n",
    "x = layers.Dense(100, activation=\"relu\")(x)\n",
    "x = layers.Dense(10, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_heaviest_dense.keras\",\n",
    " save_best_only=True)\n",
    "] \n",
    "model.compile(optimizer = \"rmsprop\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs = 5,\n",
    " validation_data = val,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_heaviest_dense.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light Model\n",
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "x = layers.Conv1D(8, 16, activation=\"relu\", padding=\"causal\")(inputs)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Conv1D(8, 8, activation=\"relu\", padding=\"causal\")(x)\n",
    "# x = layers.MaxPooling1D(2)(x)\n",
    "# x = layers.Conv1D(8, 4, activation=\"relu\")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_basic_conv1D.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=20,\n",
    " validation_data=val,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_basic_conv1D.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LSTM\n",
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_basic_LSTM.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=20,\n",
    " validation_data=val,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_basic_LSTM.keras\") \n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Stacked GRU\n",
    "do = 0.8\n",
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "x = layers.GRU(4, recurrent_dropout=do, return_sequences=True)(inputs)\n",
    "x = layers.GRU(4, recurrent_dropout=do, return_sequences=True)(x)\n",
    "x = layers.GRU(4, recurrent_dropout=do)(x)\n",
    "x = layers.Dropout(do)(x)\n",
    "x = layers.Dense(4)(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_basic_stacked_GRU.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=10,\n",
    " validation_data=val,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_basic_stacked_GRU.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models from Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nguyen (2021): (Classification?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "\n",
    "# Convolutional layer\n",
    "x = layers.Conv1D(64, 3, activation=\"relu\", padding=\"causal\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Recurrent layers\n",
    "x = layers.GRU(512, dropout = 0.5, recurrent_dropout = 0.6, return_sequences = True)(x)\n",
    "x = layers.GRU(512, dropout = 0.5, recurrent_dropout = 0.6)(x)\n",
    "\n",
    "# Output layer\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_Nguyen_GRU.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001,\n",
    "    beta_1=0.99,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-05,\n",
    ")\n",
    "model.compile(optimizer=opt, loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=10,\n",
    " validation_data=val,\n",
    " batch_size=64,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_Nguyen_GRU.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More brilliance from Nguyen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periodgram is used as input in literature, but in this case is relatively sparse of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(emgF.shape)\n",
    "\n",
    "# Raw signals\n",
    "lib.plot_emg(tF, emgF)\n",
    "fs = 1/np.mean(np.diff(tF))\n",
    "\n",
    "# Periodgram\n",
    "f, Pxx_den = signal.periodogram(emgF[:,11], fs)\n",
    "plt.semilogy(f, Pxx_den)\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.show()\n",
    "\n",
    "# Spectrogram\n",
    "f, t, Sxx = signal.spectrogram(emgF[:,10], fs, axis=0)\n",
    "print(Sxx.shape)\n",
    "# plt.pcolormesh(t, f, Sxx[:,0,:])\n",
    "plt.pcolormesh(t, f, Sxx)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "\n",
    "# Input Layer\n",
    "x = layers.Conv1D(32, 3, padding=\"causal\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x0 = layers.ReLU()(x)\n",
    "\n",
    "# Residual layer helper function\n",
    "def res(x0):\n",
    "    x1 = layers.AveragePooling1D(pool_size=2, strides=2)(x0)\n",
    "    x = layers.Conv1D(32, 3, strides = 2, padding=\"causal\")(x0)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv1D(32, 3, padding=\"causal\")(x)\n",
    "    x = layers.Add()([x, x1])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x0 = layers.ReLU()(x)\n",
    "\n",
    "    x1 = layers.AveragePooling1D(pool_size=2, strides=2)(x0)\n",
    "    x = layers.Conv1D(32, 3, strides = 1, padding=\"causal\")(x0)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv1D(32, 3, padding=\"causal\")(x)\n",
    "    x = layers.Add()([x, x0])\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x0 = layers.Dropout(0.1)(x)\n",
    "    return x0\n",
    "\n",
    "# Residual layers (Nguyen uses five, but has larger input dimensions)\n",
    "x0 = res(x0)\n",
    "x0 = res(x0)\n",
    "\n",
    "# Recurrent layers\n",
    "x = layers.GRU(64, dropout = 0.2, recurrent_dropout = 0, return_sequences = True)(x0)\n",
    "x1 = layers.GRU(64, dropout = 0.2, recurrent_dropout = 0)(x)\n",
    "\n",
    "# Attention layers\n",
    "x = layers.Dense(64, activation=\"relu\")(x1)\n",
    "x = layers.Dense(1)(x)\n",
    "x = layers.Multiply()([x, x1])\n",
    "x = layers.Softmax()(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(output_shape, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_Nguyen_Stacked.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01,\n",
    "    beta_1=0.99,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-05,\n",
    ")\n",
    "model.compile(optimizer=opt, loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=10,\n",
    " validation_data=val,\n",
    " batch_size=64,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_Nguyen_Stacked.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luu (2021): Regression Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "\n",
    "# Convolutional Layers\n",
    "# Layer 0\n",
    "x = layers.Conv1D(32, 3, strides = 1, padding=\"causal\")(inputs)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Layer 1\n",
    "x = layers.Conv1D(64, 3, strides = 2, padding=\"causal\")(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Layer 2\n",
    "x = layers.Conv1D(64, 3, strides = 1, padding=\"causal\")(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Layer 3\n",
    "x = layers.Conv1D(64, 3, strides = 1, padding=\"causal\")(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Layer 4\n",
    "x = layers.Conv1D(50, 3, strides = 1, padding=\"causal\")(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "# Output\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_Luu_Conv1D.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.005,\n",
    "    beta_1=0.99,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-05,\n",
    ")\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=5,\n",
    " validation_data=val,\n",
    " batch_size=64,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_Luu_Conv1D.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luu (2021): Regression Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1]))\n",
    "\n",
    "# Convolutional Layers\n",
    "# Layer 0\n",
    "x = layers.Conv1D(32, 3, strides = 1, padding=\"causal\")(inputs)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Layer 1\n",
    "x = layers.Conv1D(64, 3, strides = 2, padding=\"causal\")(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "# Layer 2\n",
    "x = layers.Conv1D(64, 3, strides = 1, padding=\"causal\")(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "x = layers.LSTM(64, dropout = 0.2, recurrent_dropout = 0, return_sequences = True)(x)\n",
    "outputs = layers.LSTM(output_shape, dropout = 0.2, recurrent_dropout = 0)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_Luu_RNN.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.005,\n",
    "    beta_1=0.99,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-05,\n",
    ")\n",
    "model.compile(optimizer=opt, loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=10,\n",
    " validation_data=val,\n",
    " batch_size=38,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_Luu_RNN.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lin (2022): (Classification?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape = (n_features, emgModel.shape[-1], 1))\n",
    "\n",
    "# Convolutional Layers\n",
    "# Layer 0\n",
    "x = layers.Conv2D(32, (3,int(sequence_length/10)), padding=\"valid\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.PReLU()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.MaxPooling2D(pool_size=(1,3))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.PReLU()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Conv2D(32, (3,int(sequence_length/10)), padding=\"valid\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.PReLU()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.MaxPooling2D(pool_size=(1,3))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.PReLU()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(500)(x)\n",
    "x = layers.ReLU()(x)\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_Lin_Conv2D.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.005,\n",
    "    beta_1=0.99,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-05,\n",
    ")\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=10,\n",
    " validation_data=val,\n",
    " batch_size=38,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_Lin_Conv2D.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hajin (2022): CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, input_shape,1))\n",
    "\n",
    "# Convolutional Stack 0\n",
    "# Convolutional Layer 0\n",
    "x = layers.Conv2D(128, (3,3), padding=\"same\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x = layers.Conv2D(128, (3,3), padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "# Convolutional Layer 2\n",
    "x = layers.Conv2D(256, (3,3), padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# Convolutional Layer 3\n",
    "x = layers.Conv2D(256, (3,3), padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "\n",
    "# Convolutional Layer 4\n",
    "x = layers.Conv2D(256, (3,3), padding=\"same\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output Layer\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Convolutional Stack 1\n",
    "# Convolutional Layer 0\n",
    "x1 = layers.Conv2D(128, (7,7), padding=\"same\")(inputs)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ReLU()(x1)\n",
    "\n",
    "# Convolutional Layer 1\n",
    "x1 = layers.Conv2D(128, (7,7), padding=\"same\")(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ReLU()(x1)\n",
    "x1 = layers.MaxPooling2D(pool_size=(2,2))(x1)\n",
    "\n",
    "# Convolutional Layer 2\n",
    "x1 = layers.Conv2D(256, (7,7), padding=\"same\")(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ReLU()(x1)\n",
    "\n",
    "# Convolutional Layer 3\n",
    "x1 = layers.Conv2D(256, (7,7), padding=\"same\")(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ReLU()(x1)\n",
    "x1 = layers.MaxPooling2D(pool_size=(2,2))(x1)\n",
    "\n",
    "# Convolutional Layer 4\n",
    "x1 = layers.Conv2D(256, (7,7), padding=\"same\")(x1)\n",
    "x1 = layers.BatchNormalization()(x1)\n",
    "x1 = layers.ReLU()(x1)\n",
    "x1 = layers.MaxPooling2D(pool_size=(2,2))(x1)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "# Output layer\n",
    "x1 = layers.Flatten()(x1)\n",
    "\n",
    "# Aggregation layer\n",
    "x = layers.Concatenate()([x, x1])\n",
    "x = layers.Dense(100)(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = layers.Dense(50)(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "outputs = layers.Dense(output_shape)(x)\n",
    "\n",
    "# Construct model and descent algorithm, train, and print test results\n",
    "model = keras.Model(inputs, outputs)\n",
    "callbacks = [\n",
    " keras.callbacks.ModelCheckpoint(save_dir + \"27_Hajin_Parallel_Stacked.keras\",\n",
    " save_best_only=True)\n",
    "]\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-05,\n",
    ")\n",
    "model.compile(optimizer='adam', loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model.fit(trn,\n",
    " epochs=15,\n",
    " validation_data=val,\n",
    " batch_size=128,\n",
    " callbacks=callbacks)\n",
    "model = keras.models.load_model(save_dir + \"27_Hajin_Parallel_Stacked.keras\")\n",
    "print(f\"Test RMSE: {model.evaluate(tst)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.model_summary(model, history, tst)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e02f9d08121485d963fbed69293f172ace8149a9f65bc36b8a8652d469531d6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
